"""
Service de g√©n√©ration de texte narratif avec IA

Ce service g√®re la g√©n√©ration de contenu narratif interactif en utilisant
des mod√®les de langage (Transformers). Il est responsable de :
- Chargement et gestion du mod√®le de langage configur√©
- Construction de prompts contextuels sophistiqu√©s
- G√©n√©ration de texte narratif coh√©rent et engageant
- Maintien de la coh√©rence narrative sur plusieurs sc√®nes
- Adaptation du style selon le genre d'histoire

Architecture :
- Singleton pattern pour √©viter de recharger le mod√®le
- Cache des prompts pour optimisation
- Gestion robuste des erreurs et fallbacks
"""

import asyncio
import time
from typing import Dict, List, Optional, Tuple
from datetime import datetime

try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("Warning: transformers not available. Using mock responses.")

from app.models.schemas import (
    Story, StoryMemory, Scene, UserAction, StoryGenre, Character, Location
)
from app.config import settings


class TextGenerationService:
    """
    Service de g√©n√©ration de texte narratif avec mod√®les de langage
    
    Ce service encapsule toute la logique de g√©n√©ration de contenu narratif :
    - Gestion du mod√®le de langage (chargement, optimisation)
    - Construction de prompts contextuels
    - G√©n√©ration de texte avec param√®tres adapt√©s
    - Extraction d'informations narratives (personnages, lieux)
    """
    
    _instance = None
    _model = None
    _tokenizer = None
    _pipeline = None
    _model_loaded = False
    
    def __new__(cls):
        """
        Impl√©mentation du pattern Singleton
        √âvite de recharger le mod√®le √† chaque instanciation
        """
        if cls._instance is None:
            cls._instance = super(TextGenerationService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        """
        Initialise le service de g√©n√©ration de texte
        Ne charge le mod√®le que si ce n'est pas d√©j√† fait (Singleton)
        """
        if not hasattr(self, '_initialized'):
            self._initialized = True
            
            # D√©tection automatique du device
            self.device = self._detect_device(settings.TEXT_MODEL_DEVICE)
            self.model_name = settings.TEXT_MODEL_NAME
            self.max_context_length = settings.MAX_CONTEXT_LENGTH
            self.max_story_length = settings.MAX_STORY_LENGTH
            
            print(f"üîß Service de texte configur√© - Device: {self.device}, Mod√®le: {self.model_name}")
            
            # Param√®tres de g√©n√©ration par d√©faut
            self.generation_params = {
                "max_new_tokens": 300,
                "temperature": 0.8,
                "top_p": 0.9,
                "top_k": 50,
                "do_sample": True,
                "pad_token_id": None,  # Sera d√©fini apr√®s chargement du tokenizer
                "eos_token_id": None,
                "repetition_penalty": 1.1
            }
            
            # Templates de prompts par genre
            self._genre_prompts = self._initialize_genre_prompts()
            
            # Cache des prompts r√©cents (optimisation)
            self._prompt_cache = {}
            self._cache_max_size = 100
    
    def _detect_device(self, device_setting: str) -> str:
        """
        D√©tecte automatiquement le meilleur device disponible
        
        Args:
            device_setting: Configuration du device ('auto', 'cuda', 'cpu')
            
        Returns:
            str: Device √† utiliser ('cuda' ou 'cpu')
        """
        if device_setting.lower() == "auto":
            if TRANSFORMERS_AVAILABLE and torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
                print(f"üéÆ CUDA d√©tect√©! GPU disponibles: {gpu_count}, Nom: {gpu_name}")
                return "cuda"
            else:
                print("üíª CUDA non disponible, utilisation du CPU")
                return "cpu"
        else:
            return device_setting.lower()
    
    async def initialize_model(self) -> bool:
        """
        Charge le mod√®le de langage de mani√®re asynchrone
        
        Returns:
            bool: True si le mod√®le est charg√© avec succ√®s
        """
        if self._model_loaded:
            return True
        
        if not TRANSFORMERS_AVAILABLE:
            print("Transformers non disponible. Mode simulation activ√©.")
            self._model_loaded = True
            return True
        
        try:
            print(f"Chargement du mod√®le {self.model_name}...")
            start_time = time.time()
            
            # Chargement du tokenizer
            self._tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # Configuration du padding token si n√©cessaire
            if self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token
            
            # Chargement du mod√®le avec optimisations
            model_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.float16 if self.device == "cuda" else torch.float32,
                "low_cpu_mem_usage": True
            }
            
            # Ne pas utiliser device_map="auto" qui cause des probl√®mes
            if self.device == "cuda":
                model_kwargs["device_map"] = {"": 0}  # Utiliser GPU 0 explicitement
            
            self._model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **model_kwargs
            )
            
            # D√©placer vers le device appropri√© si CPU
            if self.device == "cpu":
                self._model = self._model.to(self.device)
            
            # Cr√©ation du pipeline de g√©n√©ration
            self._pipeline = pipeline(
                "text-generation",
                model=self._model,
                tokenizer=self._tokenizer,
                device=0 if self.device == "cuda" else -1
            )
            
            # Mise √† jour des param√®tres de g√©n√©ration
            self.generation_params["pad_token_id"] = self._tokenizer.pad_token_id
            self.generation_params["eos_token_id"] = self._tokenizer.eos_token_id
            
            load_time = time.time() - start_time
            print(f"Mod√®le charg√© avec succ√®s en {load_time:.2f}s")
            
            self._model_loaded = True
            return True
            
        except Exception as e:
            print(f"Erreur lors du chargement du mod√®le: {str(e)}")
            print("Basculement vers le mode simulation...")
            self._model_loaded = True  # Mode d√©grad√©
            return False
    
    async def generate_intro_scene(self, story: Story) -> Tuple[str, List[str]]:
        """
        G√©n√®re la sc√®ne d'introduction d'une histoire
        
        Args:
            story: Histoire pour laquelle g√©n√©rer l'introduction
            
        Returns:
            Tuple[str, List[str]]: (texte_narratif, actions_sugg√©r√©es)
        """
        try:
            # Construction du prompt d'introduction
            prompt = self._build_intro_prompt(story)
            
            # G√©n√©ration du texte
            narrative_text = await self._generate_text(prompt, story.genre)
            
            # G√©n√©ration des actions sugg√©r√©es
            suggested_actions = await self._generate_suggested_actions(
                narrative_text, story.genre, is_intro=True
            )
            
            return narrative_text, suggested_actions
            
        except Exception as e:
            print(f"Erreur g√©n√©ration intro: {str(e)}")
            return self._fallback_intro(story), self._fallback_actions(story.genre)
    
    async def generate_continuation(self, story: Story, user_action: UserAction) -> Tuple[str, List[str]]:
        """
        G√©n√®re la continuation d'une histoire bas√©e sur l'action du joueur
        
        Args:
            story: Histoire en cours
            user_action: Action effectu√©e par le joueur
            
        Returns:
            Tuple[str, List[str]]: (texte_narratif, actions_sugg√©r√©es)
        """
        try:
            # Construction du prompt de continuation
            prompt = self._build_continuation_prompt(story, user_action)
            
            # G√©n√©ration du texte narratif
            narrative_text = await self._generate_text(prompt, story.genre)
            
            # G√©n√©ration des actions sugg√©r√©es
            suggested_actions = await self._generate_suggested_actions(
                narrative_text, story.genre, is_intro=False
            )
            
            return narrative_text, suggested_actions
            
        except Exception as e:
            print(f"Erreur g√©n√©ration continuation: {str(e)}")
            return self._fallback_continuation(user_action), self._fallback_actions(story.genre)
    
    async def _generate_text(self, prompt: str, genre: StoryGenre) -> str:
        """
        G√©n√®re du texte √† partir d'un prompt
        
        Args:
            prompt: Prompt √† utiliser pour la g√©n√©ration
            genre: Genre de l'histoire (pour adapter les param√®tres)
            
        Returns:
            str: Texte g√©n√©r√© et nettoy√©
        """
        if not self._model_loaded:
            await self.initialize_model()
        
        if not TRANSFORMERS_AVAILABLE or self._pipeline is None:
            return self._simulate_generation(prompt, genre)
        
        try:
            # Adaptation des param√®tres selon le genre
            params = self.generation_params.copy()
            params.update(self._get_genre_generation_params(genre))
            
            # G√©n√©ration
            start_time = time.time()
            result = self._pipeline(
                prompt,
                max_new_tokens=params["max_new_tokens"],
                temperature=params["temperature"],
                top_p=params["top_p"],
                top_k=params["top_k"],
                do_sample=params["do_sample"],
                pad_token_id=params["pad_token_id"],
                eos_token_id=params["eos_token_id"],
                repetition_penalty=params["repetition_penalty"]
            )
            
            generation_time = time.time() - start_time
            print(f"G√©n√©ration effectu√©e en {generation_time:.2f}s")
            
            # Extraction et nettoyage du texte g√©n√©r√©
            generated_text = result[0]["generated_text"]
            clean_text = self._clean_generated_text(generated_text, prompt)
            
            return clean_text
            
        except Exception as e:
            print(f"Erreur lors de la g√©n√©ration: {str(e)}")
            return self._simulate_generation(prompt, genre)
    
    def _build_intro_prompt(self, story: Story) -> str:
        """
        Construit le prompt d'introduction pour une histoire
        
        Args:
            story: Histoire √† initialiser
            
        Returns:
            str: Prompt d'introduction format√©
        """
        genre_template = self._genre_prompts[story.genre]
        
        prompt = f"""<|im_start|>system
{genre_template['system_prompt']}

Tu dois cr√©er l'introduction d'une histoire interactive {story.genre.value}.
R√®gles importantes :
- √âcris √† la 2√®me personne du singulier ("vous")
- Cr√©e une sc√®ne immersive et engageante
- Termine par une situation n√©cessitant un choix du joueur
- Reste dans le genre {story.genre.value}
- Maximum 200 mots<|im_end|>

<|im_start|>user
G√©n√®re l'introduction d'une histoire {story.genre.value}."""
        
        if story.initial_prompt:
            prompt += f"\nContexte initial fourni par l'utilisateur : {story.initial_prompt}"
        
        prompt += "<|im_end|>\n\n<|im_start|>assistant\n"
        
        return prompt
    
    def _build_continuation_prompt(self, story: Story, user_action: UserAction) -> str:
        """
        Construit le prompt de continuation bas√© sur l'historique et l'action
        
        Args:
            story: Histoire en cours
            user_action: Derni√®re action du joueur
            
        Returns:
            str: Prompt de continuation format√©
        """
        genre_template = self._genre_prompts[story.genre]
        
        # Construction du contexte narratif
        context = self._build_story_context(story)
        
        prompt = f"""<|im_start|>system
{genre_template['system_prompt']}

Tu continues une histoire interactive {story.genre.value}.
R√®gles importantes :
- √âcris √† la 2√®me personne du singulier ("vous")
- Prends en compte l'action du joueur
- Fais progresser l'histoire de mani√®re logique
- Termine par une nouvelle situation n√©cessitant un choix
- Reste coh√©rent avec le contexte pr√©c√©dent
- Maximum 250 mots<|im_end|>

<|im_start|>user
Contexte de l'histoire :
{context}

Action du joueur : {user_action.action_text}

Continue l'histoire en tenant compte de cette action.<|im_end|>

<|im_start|>assistant
"""
        
        return prompt
    
    def _build_story_context(self, story: Story) -> str:
        """
        Construit un r√©sum√© contextuel de l'histoire pour les prompts
        
        Args:
            story: Histoire dont extraire le contexte
            
        Returns:
            str: Contexte format√© pour inclusion dans les prompts
        """
        context_parts = []
        
        # R√©sum√© g√©n√©ral
        if story.memory.current_summary:
            context_parts.append(f"R√©sum√© : {story.memory.current_summary}")
        
        # Personnages importants
        if story.memory.characters:
            chars = ", ".join([f"{name} ({char.role})" 
                             for name, char in story.memory.characters.items()])
            context_parts.append(f"Personnages : {chars}")
        
        # Lieux visit√©s
        if story.memory.locations:
            locs = ", ".join(story.memory.locations.keys())
            context_parts.append(f"Lieux : {locs}")
        
        # Derni√®res sc√®nes (pour continuit√© imm√©diate)
        if story.scenes:
            recent_scenes = story.scenes[-2:] if len(story.scenes) > 1 else story.scenes[-1:]
            for scene in recent_scenes:
                context_parts.append(f"Sc√®ne {scene.scene_number} : {scene.narrative_text[:100]}...")
        
        return "\n".join(context_parts)
    
    async def _generate_suggested_actions(self, narrative_text: str, genre: StoryGenre, is_intro: bool = False) -> List[str]:
        """
        G√©n√®re des actions sugg√©r√©es bas√©es sur le contexte narratif
        
        Args:
            narrative_text: Texte narratif de la sc√®ne
            genre: Genre de l'histoire
            is_intro: Si c'est la sc√®ne d'introduction
            
        Returns:
            List[str]: Actions sugg√©r√©es pour le joueur
        """
        # Pour l'instant, utilisation de templates pr√©-d√©finis
        # TODO: Am√©liorer avec g√©n√©ration IA contextuelle
        
        if is_intro:
            return self._get_intro_actions(genre)
        else:
            return self._get_generic_actions(genre)
    
    def _get_intro_actions(self, genre: StoryGenre) -> List[str]:
        """Actions sugg√©r√©es pour les sc√®nes d'introduction par genre"""
        actions_by_genre = {
            StoryGenre.FANTASY: [
                "Explorer les environs magiques",
                "Chercher d'autres cr√©atures intelligentes",
                "Examiner vos possessions",
                "Essayer de comprendre comment vous √™tes arriv√© ici"
            ],
            StoryGenre.SCIENCE_FICTION: [
                "Consulter les syst√®mes du vaisseau",
                "Analyser la plan√®te par les scanners",
                "Chercher d'autres survivants",
                "Examiner votre √©quipement"
            ],
            StoryGenre.HORROR: [
                "Chercher une source de lumi√®re",
                "Explorer prudemment la premi√®re pi√®ce",
                "√âcouter attentivement les bruits",
                "Chercher une sortie"
            ],
            StoryGenre.MYSTERY: [
                "Examiner les indices disponibles",
                "Interroger les t√©moins potentiels",
                "Explorer la sc√®ne de crime",
                "Consulter vos notes"
            ],
            StoryGenre.ADVENTURE: [
                "√âtudier la carte et planifier votre route",
                "V√©rifier votre √©quipement",
                "Chercher des compagnons de voyage",
                "Commencer imm√©diatement l'aventure"
            ],
            StoryGenre.ROMANCE: [
                "Engager la conversation",
                "Observer discr√®tement la situation",
                "Prendre le temps de r√©fl√©chir",
                "Agir selon votre instinct"
            ]
        }
        
        return actions_by_genre.get(genre, [
            "Explorer la situation",
            "R√©fl√©chir aux options",
            "Agir prudemment",
            "Prendre une d√©cision rapide"
        ])
    
    def _get_generic_actions(self, genre: StoryGenre) -> List[str]:
        """Actions sugg√©r√©es g√©n√©riques par genre"""
        return [
            "Continuer dans cette direction",
            "Changer d'approche",
            "Examiner les d√©tails",
            "Demander de l'aide ou des informations"
        ]
    
    def _get_genre_generation_params(self, genre: StoryGenre) -> Dict:
        """
        Param√®tres de g√©n√©ration adapt√©s par genre
        
        Args:
            genre: Genre de l'histoire
            
        Returns:
            Dict: Param√®tres de g√©n√©ration sp√©cifiques
        """
        genre_params = {
            StoryGenre.HORROR: {
                "temperature": 0.7,  # Plus d√©terministe pour suspense
                "top_p": 0.85
            },
            StoryGenre.MYSTERY: {
                "temperature": 0.6,  # Logique et coh√©rent
                "top_p": 0.8
            },
            StoryGenre.ROMANCE: {
                "temperature": 0.9,  # Plus cr√©atif et √©motionnel
                "top_p": 0.95
            }
        }
        
        return genre_params.get(genre, {})
    
    def _clean_generated_text(self, generated_text: str, original_prompt: str) -> str:
        """
        Nettoie le texte g√©n√©r√© en retirant le prompt et les √©l√©ments ind√©sirables
        
        Args:
            generated_text: Texte brut g√©n√©r√©
            original_prompt: Prompt original √† retirer
            
        Returns:
            str: Texte nettoy√©
        """
        # Retirer le prompt original
        if original_prompt in generated_text:
            clean_text = generated_text.replace(original_prompt, "")
        else:
            clean_text = generated_text
        
        # Nettoyer les tokens sp√©ciaux
        clean_text = clean_text.replace("<|im_start|>", "")
        clean_text = clean_text.replace("<|im_end|>", "")
        clean_text = clean_text.replace("<|assistant|>", "")
        
        # Nettoyer les espaces multiples et retours √† la ligne
        clean_text = " ".join(clean_text.split())
        
        # Limiter la longueur si n√©cessaire
        if len(clean_text) > 500:
            clean_text = clean_text[:500] + "..."
        
        return clean_text.strip()
    
    def _initialize_genre_prompts(self) -> Dict[StoryGenre, Dict[str, str]]:
        """
        Initialise les templates de prompts par genre
        
        Returns:
            Dict: Templates structur√©s par genre
        """
        return {
            StoryGenre.FANTASY: {
                "system_prompt": "Tu es un ma√Ætre conteur sp√©cialis√© dans la fantasy. Tu cr√©√©s des histoires riches en magie, cr√©atures fantastiques, qu√™tes √©piques et mondes imaginaires. Ton style est immersif, descriptif et respecte les codes du genre fantasy."
            },
            StoryGenre.SCIENCE_FICTION: {
                "system_prompt": "Tu es un narrateur expert en science-fiction. Tu explores les technologies futuristes, l'espace, les voyages temporels, les intelligences artificielles et les concepts scientifiques avanc√©s. Ton style est innovant et sp√©culatif."
            },
            StoryGenre.HORROR: {
                "system_prompt": "Tu es un ma√Ætre de l'horreur. Tu cr√©√©s des atmosph√®res oppressantes, du suspense psychologique, des cr√©atures terrifiantes et des situations angoissantes. Ton style privil√©gie la tension et l'ambiance plut√¥t que la violence gratuite."
            },
            StoryGenre.MYSTERY: {
                "system_prompt": "Tu es un expert en r√©cits policiers et myst√®res. Tu tisses des intrigues complexes, des enqu√™tes logiques, des indices subtils et des r√©v√©lations surprenantes. Ton style est m√©thodique et respecte la logique d√©ductive."
            },
            StoryGenre.ADVENTURE: {
                "system_prompt": "Tu es un guide d'aventures extraordinaires. Tu cr√©√©s des qu√™tes passionnantes, des d√©fis physiques, des d√©couvertes fascinantes et des voyages √©piques. Ton style est dynamique et plein d'action."
            },
            StoryGenre.ROMANCE: {
                "system_prompt": "Tu es un conteur de romances touchantes. Tu d√©veloppes des relations √©motionnelles profondes, des histoires d'amour complexes, des moments tendres et des conflits relationnels. Ton style est sensible et √©motionnellement riche."
            }
        }
    
    # ===== M√âTHODES DE FALLBACK (MODE D√âGRAD√â) =====
    
    def _simulate_generation(self, prompt: str, genre: StoryGenre) -> str:
        """
        Simulation de g√©n√©ration quand le mod√®le n'est pas disponible
        
        Args:
            prompt: Prompt demand√©
            genre: Genre de l'histoire
            
        Returns:
            str: Texte simul√©
        """
        simulations = {
            StoryGenre.FANTASY: "La magie cr√©pite dans l'air tandis que vous avancez dans cette terre myst√©rieuse. Des cr√©atures √©tranges observent vos mouvements depuis les ombres de la for√™t enchant√©e. Que d√©cidez-vous de faire ?",
            StoryGenre.SCIENCE_FICTION: "Les syst√®mes du vaisseau clignotent d'alertes rouges. Par le hublot, cette plan√®te inconnue semble receler des secrets que l'humanit√© n'a jamais d√©couverts. Votre prochaine action pourrait changer le cours de l'histoire.",
            StoryGenre.HORROR: "Un frisson glac√© parcourt votre √©chine. Les ombres semblent bouger d'elles-m√™mes dans cette maison abandonn√©e. Chaque craquement du parquet vous met les nerfs √† vif. Il faut prendre une d√©cision rapidement.",
            StoryGenre.MYSTERY: "Les indices s'accumulent mais le puzzle reste incomplet. Cette affaire cache quelque chose de plus profond que pr√©vu. Votre instinct de d√©tective vous dit que vous √™tes sur la bonne piste.",
            StoryGenre.ADVENTURE: "L'adr√©naline monte tandis que vous faites face √† ce nouveau d√©fi. L'aventure que vous recherchiez se pr√©sente enfin √† vous, mais elle sera plus p√©rilleuse que pr√©vu.",
            StoryGenre.ROMANCE: "Votre c≈ìur s'acc√©l√®re dans cette situation inattendue. Les √©motions se m√™lent et il devient difficile de distinguer ce que vous ressentez vraiment. Un choix s'impose √† vous."
        }
        
        return simulations.get(genre, "L'histoire continue de se d√©velopper d'une mani√®re inattendue...")
    
    def _fallback_intro(self, story: Story) -> str:
        """Texte d'introduction de fallback"""
        return f"Bienvenue dans cette aventure {story.genre.value} ! Votre histoire commence maintenant dans cet univers fascinant. Que souhaitez-vous faire ?"
    
    def _fallback_continuation(self, user_action: UserAction) -> str:
        """Texte de continuation de fallback"""
        return f"Vous d√©cidez de {user_action.action_text.lower()}. Cette action a des cons√©quences int√©ressantes sur la suite de votre aventure..."
    
    def _fallback_actions(self, genre: StoryGenre) -> List[str]:
        """Actions de fallback par genre"""
        return [
            "Explorer les environs",
            "Chercher des informations",
            "Prendre une d√©cision importante",
            "Continuer prudemment"
        ]
    
    def get_model_status(self) -> Dict[str, any]:
        """
        Retourne l'√©tat actuel du mod√®le
        
        Returns:
            Dict: Informations sur l'√©tat du mod√®le
        """
        return {
            "model_loaded": self._model_loaded,
            "model_name": self.model_name,
            "device": self.device,
            "transformers_available": TRANSFORMERS_AVAILABLE,
            "cache_size": len(self._prompt_cache)
        }