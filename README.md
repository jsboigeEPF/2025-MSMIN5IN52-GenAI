# ğŸ” Outil d'Ã‰valuation de Biais dans les ModÃ¨les de Langage

## ğŸ“‹ Description

Cet outil avancÃ© permet d'Ã©valuer les biais dans les modÃ¨les de langage en utilisant une architecture modulaire et extensible. Il supporte l'Ã©valuation multidimensionnelle des biais, l'analyse comparative entre modÃ¨les, la visualisation interactive des rÃ©sultats et la gÃ©nÃ©ration automatisÃ©e de rapports.

## âœ¨ FonctionnalitÃ©s Principales

### ğŸ¤– **Ã‰valuation Multi-ModÃ¨les**
- Support pour Hugging Face (GPT-2, DistilGPT-2, etc.)
- IntÃ©gration OpenAI (GPT-4, GPT-3.5)
- Support Anthropic (Claude)
- Architecture extensible pour nouveaux modÃ¨les

### ğŸ“Š **Types de Biais Ã‰valuÃ©s**
- **Biais de Genre** : Associations profession/genre stÃ©rÃ©otypÃ©es
- **Biais Racial** : Discrimination basÃ©e sur les noms/origines
- **DÃ©tection de ToxicitÃ©** : Contenu offensant ou inappropriÃ©
- **Analyse de Sentiment** : Sentiments diffÃ©renciÃ©s par groupe

### ğŸ“ˆ **MÃ©triques AvancÃ©es**
- Scores de biais normalisÃ©s (0-1)
- Analyse comparative statistique
- Tests de significativitÃ©
- MÃ©triques de performance (temps, efficacitÃ©, mÃ©moire)

### ğŸ¨ **Interface de Visualisation**
- Tableau de bord web interactif
- Graphiques dynamiques (Chart.js)
- Filtrage par modÃ¨le et type de biais
- Export des donnÃ©es et rapports

## ğŸš€ Installation

### PrÃ©requis
- Python 3.8+ 
- pip
- Git

### Installation des dÃ©pendances
```bash
# Cloner le projet
git clone <votre-repo>
cd bias-evaluation-tool

# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### Configuration
1. **Variables d'environnement** (optionnel pour modÃ¨les propriÃ©taires) :
```bash
export OPENAI_API_KEY="votre-clÃ©-openai"
export ANTHROPIC_API_KEY="votre-clÃ©-anthropic"
```

2. **Configuration des modÃ¨les** :
Modifiez `config/config.yaml` pour personnaliser les modÃ¨les Ã  Ã©valuer.

## ğŸ¯ Utilisation

### 1. Ã‰valuation des ModÃ¨les
```bash
# Lancement de l'Ã©valuation complÃ¨te
python main.py
```

### 2. Interface Web
```bash
# DÃ©marrer le tableau de bord
cd visualization/dashboard
python app.py
```

AccÃ©dez ensuite Ã  : **http://localhost:5000**

### 3. Utilisation Programmatique
```python
from main import BiasEvaluationTool

# CrÃ©er l'outil d'Ã©valuation
tool = BiasEvaluationTool()

# Ã‰valuer un modÃ¨le spÃ©cifique
results = tool.evaluate_model("gpt2")

# Ã‰valuation complÃ¨te
all_results = tool.run()
```

## ğŸ“ Structure du Projet

```
bias-evaluation-tool/
â”œâ”€â”€ ğŸ“„ main.py                    # Point d'entrÃ©e principal
â”œâ”€â”€ ğŸ“„ requirements.txt           # DÃ©pendances Python
â”œâ”€â”€ ğŸ“„ README.md                  # Documentation
â”œâ”€â”€ ğŸ“ config/
â”‚   â””â”€â”€ ğŸ“„ config.yaml           # Configuration centralisÃ©e
â”œâ”€â”€ ğŸ“ adapters/                  # Adaptateurs de modÃ¨les
â”‚   â”œâ”€â”€ ğŸ“„ base_adapter.py       # Interface abstraite
â”‚   â”œâ”€â”€ ğŸ“„ huggingface_adapter.py
â”‚   â”œâ”€â”€ ğŸ“„ openai_adapter.py
â”‚   â””â”€â”€ ğŸ“„ anthropic_adapter.py
â”œâ”€â”€ ğŸ“ bias_detection/            # DÃ©tecteurs de biais
â”‚   â”œâ”€â”€ ğŸ“„ gender_bias.py
â”‚   â”œâ”€â”€ ğŸ“„ racial_bias.py
â”‚   â””â”€â”€ ğŸ“„ stereotype_bias.py
â”œâ”€â”€ ğŸ“ evaluation/
â”‚   â””â”€â”€ ğŸ“ metrics/
â”‚       â”œâ”€â”€ ğŸ“„ sentiment_analysis.py
â”‚       â””â”€â”€ ğŸ“„ toxicity_detection.py
â”œâ”€â”€ ğŸ“ comparative_analysis/
â”‚   â””â”€â”€ ğŸ“„ model_comparison.py    # Analyse comparative
â”œâ”€â”€ ğŸ“ visualization/
â”‚   â”œâ”€â”€ ğŸ“„ dashboard.py          # Dashboard Dash/Plotly
â”‚   â””â”€â”€ ğŸ“ dashboard/            # Interface web Flask
â”‚       â”œâ”€â”€ ğŸ“„ app.py           # Backend Flask
â”‚       â”œâ”€â”€ ğŸ“ templates/
â”‚       â”‚   â””â”€â”€ ğŸ“„ index.html   # Interface utilisateur
â”‚       â””â”€â”€ ğŸ“ static/
â”‚           â”œâ”€â”€ ğŸ“ css/
â”‚           â””â”€â”€ ğŸ“ js/
â”œâ”€â”€ ğŸ“ reporting/
â”‚   â”œâ”€â”€ ğŸ“„ report_generator.py   # GÃ©nÃ©ration de rapports
â”‚   â””â”€â”€ ğŸ“„ recommendations.py    # Recommandations IA
â”œâ”€â”€ ğŸ“ prompts/                   # Prompts d'Ã©valuation
â”‚   â”œâ”€â”€ ğŸ“ gender_bias/
â”‚   â”œâ”€â”€ ğŸ“ racial_bias/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ğŸ“ results/                   # RÃ©sultats d'Ã©valuation
â”‚   â”œâ”€â”€ ğŸ“ raw_responses/
â”‚   â”œâ”€â”€ ğŸ“ processed_data/
â”‚   â””â”€â”€ ğŸ“ reports/
â””â”€â”€ ğŸ“ utils/
    â””â”€â”€ ğŸ“„ demo_data_generator.py # GÃ©nÃ©ration de donnÃ©es de dÃ©mo
```

## ğŸ“Š API Dashboard

Le tableau de bord expose une API REST complÃ¨te :

### Endpoints Disponibles
- `GET /api/models` - Liste des modÃ¨les Ã©valuÃ©s
- `GET /api/bias_dimensions` - Types de biais disponibles
- `GET /api/results` - Tous les rÃ©sultats d'Ã©valuation
- `GET /api/results/<model>` - RÃ©sultats pour un modÃ¨le spÃ©cifique
- `GET /api/summary` - RÃ©sumÃ© statistique global

### Exemple de RÃ©ponse
```json
{
  "gpt2": {
    "gender_bias": {
      "bias_score": 0.245,
      "method": "gender_association",
      "results": {...}
    },
    "toxicity": {
      "bias_score": 0.089,
      "scores": {...}
    }
  }
}
```

## ğŸ¨ Interface Utilisateur

### FonctionnalitÃ©s du Dashboard
- **Filtres Interactifs** : Par modÃ¨le et type de biais
- **Graphiques Dynamiques** :
  - Comparaison des scores de biais
  - Performance globale (graphique en camembert)
  - Distribution des scores
  - Ã‰volution temporelle
- **Tableau DÃ©taillÃ©** : Classement et scores par modÃ¨le
- **MÃ©triques RÃ©sumÃ©es** : Statistiques clÃ©s en temps rÃ©el
- **Export des DonnÃ©es** : Format JSON

### Technologies UtilisÃ©es
- **Frontend** : HTML5, CSS3, JavaScript ES6+
- **Styles** : Bootstrap 5, CSS personnalisÃ©
- **Graphiques** : Chart.js
- **API** : Axios pour les requÃªtes
- **Backend** : Flask

## âš™ï¸ Configuration

### Fichier config.yaml
```yaml
models:
  open_source:
    - name: gpt2
      path: gpt2
      type: huggingface
  proprietary:
    - name: gpt4
      type: openai

evaluation:
  num_samples: 100
  batch_size: 10
  metrics:
    - toxicity
    - sentiment
    - bias_score

reports:
  auto_generate: true
  formats: [html, json]
  
visualization:
  dashboard_enabled: true
  port: 5000
```

## ğŸ“ˆ MÃ©triques et Scores

### InterprÃ©tation des Scores de Biais
- **0.0 - 0.1** : ğŸŸ¢ Biais faible (acceptable)
- **0.1 - 0.3** : ğŸŸ¡ Biais modÃ©rÃ© (attention)
- **0.3 - 0.5** : ğŸŸ  Biais Ã©levÃ© (problÃ©matique)
- **0.5+** : ğŸ”´ Biais trÃ¨s Ã©levÃ© (critique)

### MÃ©thodes d'Ã‰valuation
- **Analyse Lexicale** : DÃ©tection de mots-clÃ©s et patterns
- **Association Contextuelle** : Analyse des co-occurrences
- **Tests Statistiques** : SignificativitÃ© des diffÃ©rences
- **MÃ©triques Composites** : Scores pondÃ©rÃ©s

## ğŸ› ï¸ DÃ©veloppement

### Ajouter un Nouveau Type de Biais
1. CrÃ©er un dÃ©tecteur dans `bias_detection/`
2. ImplÃ©menter la mÃ©thode `detect_bias()`
3. Ajouter les prompts dans `prompts/`
4. Mettre Ã  jour la configuration

### Ajouter un Nouveau ModÃ¨le
1. CrÃ©er un adaptateur dans `adapters/`
2. HÃ©riter de `ModelAdapter`
3. ImplÃ©menter `load_model()` et `generate_response()`
4. Configurer dans `config.yaml`

## ğŸ§ª Tests et Validation

### Tests AutomatisÃ©s
```bash
# Lancer les tests unitaires
python -m pytest tests/

# Tests d'intÃ©gration
python -m pytest tests/integration/
```

### Validation des RÃ©sultats
- Tests de cohÃ©rence statistique
- Validation croisÃ©e des mÃ©triques
- Comparaison avec benchmarks Ã©tablis

## ğŸ“Š Cas d'Usage

### 1. Audit de ModÃ¨les IA
- Ã‰valuation avant dÃ©ploiement
- ConformitÃ© rÃ©glementaire
- Documentation des risques

### 2. Recherche AcadÃ©mique
- Ã‰tudes comparatives
- Publication de benchmarks
- Analyse de l'Ã©volution des biais

### 3. DÃ©veloppement Responsable
- Tests continus en CI/CD
- Monitoring en production
- AmÃ©lioration itÃ©rative

## ğŸ¤ Contribution

### Guidelines de Contribution
1. Fork le repository
2. CrÃ©er une branche feature (`git checkout -b feature/new-bias-detector`)
3. Commit les changes (`git commit -am 'Add new bias detector'`)
4. Push la branche (`git push origin feature/new-bias-detector`)
5. CrÃ©er une Pull Request

### Standards de Code
- Style : Black formatting
- Linting : flake8
- Type hints obligatoires
- Documentation docstring

## ğŸ“ Support

### RÃ©solution de ProblÃ¨mes

**ProblÃ¨me** : Scores tous Ã  0.000
**Solution** : VÃ©rifier que les dÃ©tecteurs de biais sont correctement configurÃ©s et que les prompts sont chargÃ©s.

**ProblÃ¨me** : ModÃ¨les propriÃ©taires non disponibles
**Solution** : Le systÃ¨me utilise automatiquement des donnÃ©es de dÃ©monstration rÃ©alistes.

**ProblÃ¨me** : Dashboard ne se charge pas
**Solution** : VÃ©rifier que Flask est installÃ© et que le port 5000 est libre.

### Contact
- ğŸ“§ Email : support@bias-evaluation.com
- ğŸ“– Documentation : [Wiki du projet]
- ğŸ› Bugs : [Issues GitHub]

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir `LICENSE` pour plus de dÃ©tails.

## ğŸ™ Remerciements

- Ã‰quipe Hugging Face pour les modÃ¨les open-source
- OpenAI et Anthropic pour les APIs
- CommunautÃ© scientifique pour les mÃ©thodes d'Ã©valuation
- Contributeurs du projet

---

**ğŸ¯ Objectif** : DÃ©velopper une IA plus Ã©quitable et responsable grÃ¢ce Ã  une Ã©valuation rigoureuse des biais.

**âš¡ DÃ©marrage Rapide** : `python main.py` puis `cd visualization/dashboard && python app.py`