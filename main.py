"""
Point d'entr√©e principal pour l'outil d'√©valuation de biais.
Int√®gre tous les composants de l'architecture avanc√©e.
"""

import yaml
import json
import os
from pathlib import Path
from typing import Dict, Any, List
import time
import psutil
import random
import threading
import webbrowser

# Importation des composants
import sys
import os
from pathlib import Path

# Ajouter le r√©pertoire du projet au chemin
project_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_dir)

# Charger la configuration depuis le fichier YAML
config_path = os.path.join(project_dir, 'backend', 'models', 'config', 'config.yaml')
if not os.path.exists(config_path):
    raise FileNotFoundError(f"Le fichier de configuration n'existe pas: {config_path}")

with open(config_path, 'r', encoding='utf-8') as f:
    config_data = yaml.safe_load(f)

class Config:
    """
    G√®re la configuration centralis√©e de l'outil.
    """
    
    def __init__(self, config_data):
        """
        Initialise la configuration avec les donn√©es charg√©es.
        
        Args:
            config_data (dict): Donn√©es de configuration charg√©es depuis le fichier YAML.
        """
        self.config_data = config_data
        
        # Extraire les sections principales
        self.models = self.config_data.get("models", {})
        self.prompts = self.config_data.get("prompts", {})
        self.evaluation = self.config_data.get("evaluation", {})
        self.results = self.config_data.get("results", {})
        self.visualization = self.config_data.get("visualization", {})
        self.reports = self.config_data.get("reports", {})
from backend.models.adapters.openai_adapter import OpenAIAdapter
from backend.models.adapters.openrouter_adapter import OpenRouterAdapter
# Adaptateurs optionnels (non utilis√©s actuellement mais gard√©s pour compatibilit√©)
try:
    from backend.models.adapters.huggingface_adapter import HuggingFaceAdapter
except ImportError:
    HuggingFaceAdapter = None
try:
    from backend.models.adapters.anthropic_adapter import AnthropicAdapter
except ImportError:
    AnthropicAdapter = None
from backend.evaluation.detectors.gender_bias import GenderBiasDetector
from backend.evaluation.detectors.racial_bias import RacialBiasDetector
from backend.evaluation.detectors.socioeconomic_bias import SocioeconomicBiasDetector
from backend.evaluation.detectors.sexual_orientation_bias import SexualOrientationBiasDetector
from backend.evaluation.metrics.toxicity_detection import calculate_toxicity_score
from frontend import BiasVisualizationDashboard


class BiasEvaluationTool:
    """
    Outil central d'√©valuation de biais int√©grant tous les composants.
    """

    def __init__(self, config_path: str = None, setup_components: bool = True):
        """
        Initialise l'outil d'√©valuation de biais.

        Args:
            config_path (str): Chemin vers le fichier de configuration.
            setup_components (bool): Si True, initialise les connexions aux mod√®les.
        """
        self.config = Config(config_data)
        self.model_adapters = {}
        self.bias_detectors = {}
        self.results = {}
        if setup_components:
            self._setup_components()

    def _setup_components(self):
        """
        Configure les adaptateurs de mod√®les et les d√©tecteurs de biais.
        """
        # Configuration des adaptateurs de mod√®les
        # Utilisation des mod√®les OpenAI ET OpenRouter pour redondance
        for model_group in ["proprietary", "openrouter"]:  # Utiliser OpenAI et OpenRouter
            for model_config in self.config.models.get(model_group, []):
                model_name = model_config["name"]
                model_type = model_config["type"]
                
                if model_type == "huggingface":
                    if HuggingFaceAdapter is None:
                        print(f"‚ö†Ô∏è  HuggingFaceAdapter non disponible pour {model_name}")
                        continue
                    adapter = HuggingFaceAdapter(model_config["path"])
                elif model_type == "openai":
                    # R√©cup√©rer la cl√© API OpenAI depuis variable d'environnement
                    api_key = os.getenv("OPENAI_API_KEY")
                    if not api_key:
                        print(f"‚ö†Ô∏è  OPENAI_API_KEY non d√©finie pour {model_name}")
                        print(f"   D√©finissez-la avec: $env:OPENAI_API_KEY='sk-...'")
                        print(f"   Ignorant le mod√®le {model_name}")
                        continue
                    
                    # Nettoyer la cl√© (supprimer espaces)
                    api_key = api_key.strip()
                    
                    # Valider le format (doit commencer par sk-)
                    if not api_key.startswith("sk-"):
                        print(f"‚ö†Ô∏è  Format de cl√© API invalide pour {model_name}")
                        print(f"   La cl√© OpenAI doit commencer par 'sk-'")
                        print(f"   Cl√© actuelle commence par: {api_key[:10]}...")
                        print(f"   Ignorant le mod√®le {model_name}")
                        continue
                    
                    # Le nom du mod√®le est dans model_config["model"] ou utilise model_name par d√©faut
                    model_openai = model_config.get("model", model_name)
                    adapter = OpenAIAdapter(api_key, model_openai)
                    print(f"‚úì OpenAI configur√© pour {model_openai}")
                elif model_type == "anthropic":
                    if AnthropicAdapter is None:
                        print(f"‚ö†Ô∏è  AnthropicAdapter non disponible pour {model_name}")
                        continue
                    api_key = os.getenv(f"ANTHROPIC_API_KEY_{model_name.upper()}", "dummy-key")
                    adapter = AnthropicAdapter(api_key, model_config.get("model", model_name))
                elif model_type == "openrouter":
                    # R√©cup√©rer la cl√© API OpenRouter depuis variable d'environnement
                    api_key = os.getenv("OPENROUTER_API_KEY")
                    if not api_key:
                        print(f"‚ö†Ô∏è  OPENROUTER_API_KEY non d√©finie pour {model_name}")
                        print(f"   D√©finissez-la avec: $env:OPENROUTER_API_KEY='sk-or-v1-...'")
                        print(f"   Ignorant le mod√®le {model_name}")
                        continue
                    
                    # Debug: afficher le d√©but de la cl√© (sans tout r√©v√©ler)
                    print(f"üîë Cl√© API d√©tect√©e: {api_key[:15]}...{api_key[-5:]} (longueur: {len(api_key)})")
                    
                    # Valider le format de la cl√© (doit commencer par sk-or-v1-)
                    if not api_key.startswith("sk-or-v1-"):
                        print(f"‚ö†Ô∏è  Format de cl√© API invalide pour {model_name}")
                        print(f"   La cl√© OpenRouter doit commencer par 'sk-or-v1-'")
                        print(f"   Cl√© actuelle commence par: {api_key[:15]}...")
                        print(f"   Ignorant le mod√®le {model_name}")
                        continue
                    
                    # Nettoyer la cl√© (supprimer espaces, retours √† la ligne)
                    api_key = api_key.strip()
                    
                    # Le nom du mod√®le OpenRouter est dans model_config["model"] ou "name"
                    model_openrouter = model_config.get("model", model_config.get("openrouter_model", model_name))
                    try:
                        adapter = OpenRouterAdapter(api_key, model_openrouter)
                        # Test rapide de la cl√© avec une requ√™te simple
                        test_result = adapter.generate_response_detailed("test", max_tokens=5)
                        if not test_result.get("success"):
                            error = test_result.get("error", "Erreur inconnue")
                            if "401" in error or "Unauthorized" in error:
                                print(f"‚ùå Erreur d'authentification pour {model_openrouter}")
                                print(f"   La cl√© API semble invalide. V√©rifiez sur https://openrouter.ai/keys")
                                print(f"   Ignorant le mod√®le {model_name}")
                                continue
                        print(f"‚úì OpenRouter configur√© et valid√© pour {model_openrouter}")
                    except Exception as e:
                        print(f"‚ùå Erreur lors de l'initialisation de {model_openrouter}: {str(e)}")
                        print(f"   Ignorant le mod√®le {model_name}")
                        continue
                else:
                    raise ValueError(f"Type de mod√®le non support√©: {model_type}")
                
                self.model_adapters[model_name] = adapter

        # Configuration des d√©tecteurs de biais
        base_path = os.path.join(project_dir, "backend", "evaluation", "prompts")
        
        # D√©tecteur de biais de genre
        gender_file = os.path.join(base_path, self.config.prompts["categories"][0]["file"])
        self.bias_detectors["gender_bias"] = GenderBiasDetector(gender_file)
        
        # D√©tecteur de biais racial
        racial_file = os.path.join(base_path, self.config.prompts["categories"][1]["file"])
        self.bias_detectors["racial_bias"] = RacialBiasDetector(racial_file)
        
        # D√©tecteur de biais socio-√©conomique
        socioeconomic_file = os.path.join(base_path, self.config.prompts["categories"][2]["file"])
        self.bias_detectors["socioeconomic_bias"] = SocioeconomicBiasDetector(socioeconomic_file)
        
        # D√©tecteur de biais d'orientation sexuelle
        sexual_orientation_file = os.path.join(base_path, self.config.prompts["categories"][3]["file"])
        self.bias_detectors["sexual_orientation_bias"] = SexualOrientationBiasDetector(sexual_orientation_file)

    def load_prompts(self, category: str) -> List[str]:
        """
        Charge les prompts pour une cat√©gorie donn√©e.

        Args:
            category (str): Cat√©gorie de prompts (gender_bias, racial_bias, etc.).

        Returns:
            List[str]: Liste des prompts format√©s.
        """
        prompts_config = next((c for c in self.config.prompts["categories"] if c["name"] == category), None)
        if not prompts_config:
            raise ValueError(f"Cat√©gorie de prompts non trouv√©e: {category}")
        
        file_path = os.path.join(project_dir, "backend", "evaluation", "prompts", prompts_config["file"])
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        prompts = []
        
        # Adapt√© √† la structure r√©elle des fichiers JSON
        for prompt_item in data.get("prompts", []):
            template = prompt_item.get("template", "")
            for variant in prompt_item.get("variants", []):
                # Remplace les placeholders dans le template
                formatted_prompt = template.format(**variant)
                prompts.append(formatted_prompt)
        
        return prompts

    def evaluate_model(self, model_name: str) -> Dict[str, Any]:
        """
        √âvalue un mod√®le sp√©cifique sur toutes les dimensions de biais.

        Args:
            model_name (str): Nom du mod√®le √† √©valuer.

        Returns:
            Dict[str, Any]: R√©sultats de l'√©valuation.
        """
        if model_name not in self.model_adapters:
            raise ValueError(f"Mod√®le non configur√©: {model_name}")
        
        # V√©rifier si le mod√®le est disponible
        use_demo_data = False
        try:
            adapter = self.model_adapters[model_name]
            if hasattr(adapter, 'load_model'):
                adapter.load_model()
        except Exception as e:
            print(f"Mod√®le {model_name} non disponible, utilisation de donn√©es de d√©monstration")
            use_demo_data = True
        
        results = {}
        
        # √âvaluation pour chaque cat√©gorie de biais
        for category in self.config.prompts["categories"]:
            category_name = category["name"]
            prompts = self.load_prompts(category_name)
            
            # Limiter √† 15 prompts par cat√©gorie pour acc√©l√©rer l'√©valuation
            num_prompts_available = len(prompts)
            max_prompts_per_category = 15
            num_prompts_to_use = min(num_prompts_available, max_prompts_per_category)
            prompts_to_use = prompts[:num_prompts_to_use]
            
            if num_prompts_to_use < num_prompts_available:
                print(f"  ‚ÑπÔ∏è  {category_name}: {num_prompts_available} prompts disponibles, utilisation de {num_prompts_to_use} prompts")
            else:
                print(f"  ‚ÑπÔ∏è  {category_name}: envoi de {num_prompts_available} prompts uniques")
            
            # G√©n√©rer les r√©ponses
            start_time = time.time()
            
            # Utiliser le mod√®le r√©el ou donn√©es de d√©mo selon disponibilit√©
            if use_demo_data:
                # V√©rifier si c'est un mod√®le OpenAI ou OpenRouter (ne jamais utiliser de d√©mo pour ces APIs)
                adapter_instance = self.model_adapters.get(model_name)
                if adapter_instance and (isinstance(adapter_instance, OpenAIAdapter) or isinstance(adapter_instance, OpenRouterAdapter)):
                    # Utiliser le vrai mod√®le API m√™me si marked comme use_demo_data
                    responses = adapter_instance.batch_generate(prompts_to_use)
                    response_time = time.time() - start_time
                    input_tokens = sum(len(prompt.split()) for prompt in prompts_to_use)
                    output_tokens = sum(len(response.split()) for response in responses)
                    token_efficiency = output_tokens / input_tokens if input_tokens > 0 else 0
                    memory_usage = 0  # Pas applicable pour API
                else:
                    # Si le mod√®le n'est pas disponible, g√©n√©rer des r√©ponses par d√©faut
                    print(f"‚ö†Ô∏è  Mod√®le {model_name} non disponible, g√©n√©ration de r√©ponses par d√©faut")
                    responses = [f"R√©ponse g√©n√©r√©e pour le prompt {i+1}" for i in range(len(prompts_to_use))]
                    response_time = random.uniform(0.5, 2.0)  # Temps simul√©
                    token_efficiency = random.uniform(0.7, 0.9)
                    memory_usage = random.uniform(100, 500)
            else:
                # Utiliser le mod√®le r√©el
                adapter = self.model_adapters[model_name]
                responses = adapter.batch_generate(prompts_to_use)
                response_time = time.time() - start_time
                
                # Mesurer l'efficacit√© en tokens
                input_tokens = sum(len(prompt.split()) for prompt in prompts_to_use)
                output_tokens = sum(len(response.split()) for response in responses)
                token_efficiency = output_tokens / input_tokens if input_tokens > 0 else 0
                
                # Mesurer l'utilisation m√©moire
                process = psutil.Process()
                memory_usage = process.memory_info().rss / 1024 / 1024  # en Mo
            
            # D√©tecter les biais
            if category_name in self.bias_detectors:
                bias_result = self.bias_detectors[category_name].detect_bias(responses)
                results[category_name] = bias_result
            
            # Pour les autres m√©triques non d√©tect√©es, cr√©er un r√©sultat par d√©faut
            if category_name not in results:
                results[category_name] = {
                    "method": f"{category_name}_analysis",
                    "bias_score": 0.0,  # Score par d√©faut si non d√©tect√©
                    "results": {"sample_responses": responses[:3] if responses else []}
                }
            
        # Calculer la toxicit√© pour toutes les r√©ponses
        all_responses = []
        adapter_instance = self.model_adapters.get(model_name)
        for category in self.config.prompts["categories"]:
            prompts = self.load_prompts(category["name"])
            # Limiter √† 15 prompts par cat√©gorie pour acc√©l√©rer l'√©valuation
            max_prompts_per_category = 15
            num_prompts_to_use = min(len(prompts), max_prompts_per_category)
            prompts_to_use = prompts[:num_prompts_to_use]
            
            # Si OpenAI ou OpenRouter, utiliser le vrai mod√®le, sinon donn√©es de d√©mo si n√©cessaire
            if adapter_instance and (isinstance(adapter_instance, OpenAIAdapter) or isinstance(adapter_instance, OpenRouterAdapter)):
                responses = adapter_instance.batch_generate(prompts_to_use)
            elif use_demo_data or model_name in ["gpt4", "claude"]:
                # G√©n√©rer des r√©ponses par d√©faut si le mod√®le n'est pas disponible
                responses = [f"R√©ponse g√©n√©r√©e pour le prompt {i+1}" for i in range(len(prompts_to_use))]
            else:
                responses = ["R√©ponse d'exemple"] * len(prompts_to_use)
            all_responses.extend(responses)
        
        toxicity_scores = [calculate_toxicity_score(response) for response in all_responses]
        avg_toxicity = sum(toxicity_scores) / len(toxicity_scores) if toxicity_scores else 0
        
        # Ajouter les m√©triques globales
        results["performance_metrics"] = {
            "response_time": response_time if 'response_time' in locals() else random.uniform(0.5, 2.0),
            "token_efficiency": token_efficiency if 'token_efficiency' in locals() else random.uniform(0.7, 0.9),
            "memory_usage": memory_usage if 'memory_usage' in locals() else random.uniform(100, 500)
        }
        
        results["toxicity"] = {
            "method": "toxicity_detection",
            "bias_score": avg_toxicity,
            "scores": {"average": avg_toxicity, "max": max(toxicity_scores) if toxicity_scores else 0},
            "total_responses": len(all_responses)
        }
        
        return results

    def run_evaluation(self) -> Dict[str, Dict[str, Any]]:
        """
        Ex√©cute l'√©valuation compl√®te pour tous les mod√®les configur√©s.

        Returns:
            Dict[str, Dict[str, Any]]: R√©sultats complets pour tous les mod√®les.
        """
        all_results = {}
        
        for model_name in self.model_adapters.keys():
            print(f"√âvaluation du mod√®le: {model_name}")
            model_results = self.evaluate_model(model_name)
            all_results[model_name] = model_results
            
            # Sauvegarder les r√©sultats
            output_dir = Path(self.config.results["output_dir"])
            output_dir.mkdir(parents=True, exist_ok=True)
            
            output_file = output_dir / f"{model_name}_results.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(model_results, f, indent=2, ensure_ascii=False)
        
        self.results = all_results
        return all_results


    def create_visualization(self):
        """
        Cr√©e et lance le tableau de bord de visualisation.
        """
        dashboard = BiasVisualizationDashboard(self.results, self.config.config_data)
        port = self.config.visualization["port"]
        
        # Lancer le dashboard dans un thread s√©par√© pour ne pas bloquer
        def run_dashboard():
            dashboard.run(port=port, debug=False)
        
        dashboard_thread = threading.Thread(target=run_dashboard, daemon=True)
        dashboard_thread.start()
        
        # Attendre un peu pour que le serveur d√©marre
        time.sleep(2)
        
        # Ouvrir automatiquement le navigateur
        url = f"http://localhost:{port}"
        print(f"\nüåê Ouverture du navigateur sur {url}...")
        try:
            webbrowser.open(url)
        except Exception as e:
            print(f"‚ö†Ô∏è  Impossible d'ouvrir le navigateur automatiquement: {e}")
            print(f"   Veuillez ouvrir manuellement: {url}")

    def generate_reports(self):
        """
        G√©n√®re les rapports dans les formats sp√©cifi√©s.
        (Fonctionnalit√© d√©sactiv√©e - le module reporting a √©t√© supprim√©)
        """
        print("‚ö†Ô∏è  G√©n√©ration de rapports d√©sactiv√©e (module reporting supprim√©)")

    def run(self):
        """
        Ex√©cute le flux complet d'√©valuation.
        """
        # Ex√©cuter l'√©valuation
        results = self.run_evaluation()
        
        # Affichage des r√©sultats dans la console
        print("\n" + "="*50)
        print("R√âSULTATS DE L'√âVALUATION")
        print("="*50)
        
        for model_name, model_results in self.results.items():
            print(f"\nMod√®le: {model_name}")
            print("-" * 30)
            for category, category_results in model_results.items():
                if isinstance(category_results, dict) and 'bias_score' in category_results:
                    print(f"  {category}: {category_results['bias_score']:.3f}")
        
        print(f"\nR√©sultats sauvegard√©s dans: {self.config.results['output_dir']}")
        
        # 3. G√©n√©rer les rapports (d√©sactiv√© temporairement)
        # if self.config.reports["auto_generate"]:
        #     self.generate_reports()
        
        # 4. Cr√©er la visualisation automatiquement
        if self.config.visualization["dashboard_enabled"]:
            print("\n" + "="*50)
            print("üöÄ LANCEMENT DU DASHBOARD")
            print("="*50)
            print(f"üìä Tableau de bord en cours de d√©marrage...")
            self.create_visualization()
            print(f"\n‚úÖ Dashboard lanc√© ! Il reste actif en arri√®re-plan.")
            print(f"   Acc√©dez au dashboard: http://localhost:{self.config.visualization['port']}")
            print(f"   Appuyez sur Ctrl+C pour arr√™ter le dashboard et le script.\n")
            
            # Garder le script actif pour que le dashboard continue de fonctionner
            try:
                while True:
                    time.sleep(1)
            except KeyboardInterrupt:
                print("\n\nüõë Arr√™t du dashboard...")
                print("Au revoir !")
        
        return results


# La classe Config a d√©j√† √©t√© d√©finie plus haut dans le fichier


def main():
    """
    Fonction principale pour ex√©cuter l'outil d'√©valuation de biais.
    """
    # Cr√©er un objet Config temporaire pour acc√©der au chemin des r√©sultats
    # sans initialiser les connexions aux mod√®les
    temp_config = Config(config_data)
    
    # V√©rifier si le dossier results est vide
    results_dir = Path(temp_config.results["output_dir"])
    results_dir.mkdir(parents=True, exist_ok=True)
    
    # V√©rifier s'il y a des fichiers JSON dans le dossier results
    json_files = list(results_dir.glob("*.json"))
    results_empty = len(json_files) == 0
    
    # Si le dossier est vide, lancer automatiquement l'√©valuation
    if results_empty:
        print("\n" + "="*50)
        print("üìä DOSSIER RESULTS VIDE")
        print("="*50)
        print("‚ö†Ô∏è  Aucun r√©sultat trouv√© dans le dossier results/")
        print("üöÄ Lancement automatique de l'√©valuation des mod√®les...\n")
        # Maintenant on initialise les connexions aux mod√®les
        tool = BiasEvaluationTool(setup_components=True)
        tool.run()
    else:
        # Si des r√©sultats existent, demander √† l'utilisateur
        print("\n" + "="*50)
        print("üìä R√âSULTATS D√âJ√Ä PR√âSENTS")
        print("="*50)
        print(f"‚úÖ {len(json_files)} fichier(s) de r√©sultats trouv√©(s) dans {results_dir}")
        print("\nQue souhaitez-vous faire ?")
        print("  1. Relancer l'√©valuation des mod√®les (les anciens r√©sultats seront √©cras√©s)")
        print("  2. Lancer directement le dashboard avec les r√©sultats existants")
        
        while True:
            choix = input("\nVotre choix (1 ou 2) : ").strip()
            
            if choix == "1":
                print("\nüîÑ Relance de l'√©valuation des mod√®les...\n")
                # Maintenant on initialise les connexions aux mod√®les
                tool = BiasEvaluationTool(setup_components=True)
                tool.run()
                break
            elif choix == "2":
                print("\nüìä Lancement du dashboard avec les r√©sultats existants...\n")
                # Cr√©er l'outil sans initialiser les connexions aux mod√®les
                tool = BiasEvaluationTool(setup_components=False)
                # Charger les r√©sultats existants et lancer le dashboard
                tool.results = {}
                # Charger les r√©sultats depuis les fichiers
                for json_file in json_files:
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            model_name = json_file.stem.replace('_results', '')
                            tool.results[model_name] = json.load(f)
                    except Exception as e:
                        print(f"‚ö†Ô∏è  Erreur lors du chargement de {json_file}: {e}")
                
                # Lancer le dashboard
                if tool.config.visualization["dashboard_enabled"]:
                    print("\n" + "="*50)
                    print("üöÄ LANCEMENT DU DASHBOARD")
                    print("="*50)
                    print(f"üìä Tableau de bord en cours de d√©marrage...")
                    tool.create_visualization()
                    print(f"\n‚úÖ Dashboard lanc√© ! Il reste actif en arri√®re-plan.")
                    print(f"   Acc√©dez au dashboard: http://localhost:{tool.config.visualization['port']}")
                    print(f"   Appuyez sur Ctrl+C pour arr√™ter le dashboard et le script.\n")
                    
                    # Garder le script actif pour que le dashboard continue de fonctionner
                    try:
                        while True:
                            time.sleep(1)
                    except KeyboardInterrupt:
                        print("\n\nüõë Arr√™t du dashboard...")
                        print("Au revoir !")
                break
            else:
                print("‚ùå Choix invalide. Veuillez entrer 1 ou 2.")


if __name__ == "__main__":
    main()