# LLM Bias Auditor

## Description du Projet
LLM Bias Auditor est un outil lÃ©ger dâ€™**audit de biais** pour modÃ¨les de langage. Il exÃ©cute une batterie de **prompts standardisÃ©s** (contreâ€‘factuels) et mesure des **Ã©carts de comportement** entre groupes (ex. formulations diffÃ©rentes mais Ã©quivalentes). Le projet propose un fournisseur `mock` (horsâ€‘ligne) pour la reproductibilitÃ© et un connecteur `openai` (optionnel) pour tester des modÃ¨les hÃ©bergÃ©s. Les rÃ©sultats sont exportÃ©s en CSV et visualisÃ©s sous forme de graphiques simples.

## Technologies ClÃ©s
- **Python** (>=3.10)
- **Pandas** â€” gestion des donnÃ©es et agrÃ©gations
- **PyYAML** â€” dÃ©finition des jeux de prompts (YAML)
- **Matplotlib** â€” visualisations (barres, moyennes par groupe)
- **VADER Sentiment** â€” score de polaritÃ© (compound)
- **Jinja2** (extensible) â€” gÃ©nÃ©ration de rapports Markdown
- **python-dotenv** â€” gestion des clÃ©s/API et variables dâ€™environnement
- **OpenAI (SDK)** â€” fournisseur optionnel (`openai`)
- **Tenacity** â€” reprises automatiques (si besoin, pour les appels API)

## FonctionnalitÃ©s Principales
- ğŸ”Œ **Couche fournisseurs** : `mock` (offline) et `openai` (optionnel).  
- ğŸ§© **Dataset de prompts** au format YAML, avec **variables** pour crÃ©er des paires de test contreâ€‘factuelles.  
- ğŸ“ **Mesures intÃ©grÃ©es** : polaritÃ© VADER, taux de refus, ratio dâ€™attÃ©nuation (hedging), longueur, indicateur nÃ©gatif.  
- ğŸ“Š **AgrÃ©gations par groupe** avec calcul dâ€™**Ã©carts vs. base** (`*_gap_vs_base`) et export des **graphiques**.  
- â™»ï¸ **Reproductible** : seed fixe, configuration centralisÃ©e, traces de chaque exÃ©cution (`run_*.csv`).

## Structure du Projet
```
llm-bias-auditor/
â”œâ”€ src/
â”‚  â”œâ”€ auditor/
â”‚  â”‚  â”œâ”€ providers/                 # Adaptateurs de fournisseurs
â”‚  â”‚  â”‚  â”œâ”€ base_provider.py
â”‚  â”‚  â”‚  â”œâ”€ mock_provider.py
â”‚  â”‚  â”‚  â””â”€ openai_provider.py
â”‚  â”‚  â”œâ”€ prompts/                   # Jeux de tests YAML
â”‚  â”‚  â”‚  â””â”€ bias_tests.yaml
â”‚  â”‚  â”œâ”€ templates/
â”‚  â”‚  â”‚  â””â”€ system_prompt.txt
â”‚  â”‚  â”œâ”€ config.py                  # Chargement de la config
â”‚  â”‚  â”œâ”€ datasets.py                # Expansion des prompts (variables â†’ combinaisons)
â”‚  â”‚  â”œâ”€ scoring.py                 # Calcul des mÃ©triques & agrÃ©gations
â”‚  â”‚  â”œâ”€ run_audit.py               # Pipeline d'audit
â”‚  â”‚  â””â”€ utils.py                   # Utilitaires (I/O, horodatage, hash)
â”‚  â””â”€ scripts/
â”‚     â”œâ”€ run_all.py                 # ExÃ©cution complÃ¨te
â”‚     â””â”€ export_report.py           # Graphiques & squelette de rapport
â”œâ”€ data/
â”‚  â”œâ”€ raw/
â”‚  â”œâ”€ processed/
â”‚  â””â”€ results/
â”‚     â”œâ”€ runs/                      # DÃ©tails d'exÃ©cution (CSV)
â”‚     â””â”€ reports/                   # Graphiques/rapports par exÃ©cution
â”œâ”€ configs/
â”‚  â””â”€ default.yaml                  # Configuration par dÃ©faut
â”œâ”€ notebooks/
â”œâ”€ tests/
â”œâ”€ .env.example
â”œâ”€ requirements.txt
â”œâ”€ Makefile
â”œâ”€ LICENSE
â””â”€ README.md
```

## Sorties et Indicateurs
### Fichiers gÃ©nÃ©rÃ©s
- **`data/results/runs/run_*.csv`** â€” dÃ©tails de chaque instance :  
  `uid, task, group, mapping, prompt, response, sentiment, refusal, length, hedge_ratio, negative`
- **`data/results/reports/<run_id>/`** â€” graphiques PNG (moyennes par groupe).

### Indicateurs calculÃ©s
- **Sentiment (VADER compound)** : score de polaritÃ© in [-1, 1].  
- **Taux de refus (`refusal`)** : dÃ©tection par expressions de refus (ex. Â«Â je ne peux pas rÃ©pondreÂ Â», Â«Â dÃ©solÃ©Â Â», etc.).  
- **Ratio dâ€™attÃ©nuation (`hedge_ratio`)** : proportion de mots dâ€™hÃ©sitation (ex. Â«Â peutâ€‘ÃªtreÂ Â», Â«Â sembleÂ Â»).  
- **Longueur (`length`)** : longueur de la rÃ©ponse (caractÃ¨res).  
- **NÃ©gatif (`negative`)** : indicateur binaire si `sentiment < -0.2`.  
- **AgrÃ©gation par groupe** : moyenne par `group` et **Ã©cart vs. base** pour chaque mÃ©trique (`*_gap_vs_base`).

